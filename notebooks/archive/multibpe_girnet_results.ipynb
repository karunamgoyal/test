{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### include useful folders\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../vendors/mtl_girnet/data_prep/\")\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "# tokenizer\n",
    "from twokenize import tokenizeRawTweetText as tokenize\n",
    "\n",
    "# for a particular dataset\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code-Mixed: en_es_wssa_data: 3062\n",
      "Spanish: es2_twitter_data: 3202\n",
      "Spanish: es_tass1_data: 7217\n",
      "English: en_twitter_data: 4241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### SemEval 2017 Task A\n",
    "\n",
    "df = pd.read_csv(\"../data/datastories-semeval2017-task4/dataset/Subtask_A/4A-English/SemEval2017-task4-dev.subtask-A.english.INPUT.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "decode_map = {\"negative\": -1, \"neutral\": 0, \"positive\": 1}\n",
    "\n",
    "df[1] = df[1].apply(lambda x: decode_map[x])\n",
    "df[2] = df[2].apply(lambda x: tokenize(x))\n",
    "\n",
    "data = map( lambda x :{'sentiment': x[1] , 'tokens': x[2] , 'text': ' '.join(x[2])} , df.to_numpy() )\n",
    "\n",
    "en_semeval_17 = list(data)\n",
    "\n",
    "\n",
    "### English-Spanish Code Mixed Data \n",
    "\n",
    "sents = {\"N\":-1 , \"P\" :1 , \"NONE\":0}\n",
    "\n",
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_train.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_train = list(data)\n",
    "\n",
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/cs-corpus-with-tweets_test.txt\", encoding='utf-8').read().split(\"\\n\") \n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': sents[x[1]] , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "en_es_wssa_data_test = list(data)\n",
    "\n",
    "en_es_wssa_data = list(en_es_wssa_data_train) + list(en_es_wssa_data_test)\n",
    "\n",
    "### Spanish Tweet Dataset\n",
    "\n",
    "xmldoc = minidom.parse(\"../vendors/mtl_girnet/data_prep/data_cm_senti/general-tweets-train-tagged.xml\")\n",
    "tweets = xmldoc.getElementsByTagName('tweet')\n",
    "\n",
    "sents = {\"N\":-1 , \"P\" :1 , \"NEU\":0 , 'NONE':0 , \"P+\" : 1 , \"N+\":-1 }\n",
    "\n",
    "\n",
    "es_tass1_data = []\n",
    "\n",
    "for i in range( len(tweets)-1) :\n",
    "    if i == 6055:\n",
    "        continue # bad jogar\n",
    "    textt = tweets[i].getElementsByTagName('content')[0].childNodes[0].data\n",
    "    words = tokenize( textt )\n",
    "    sentiment = tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('value')[0].childNodes[0].data\n",
    "    assert len(tweets[i].getElementsByTagName('polarity')[0].getElementsByTagName('entity'))==0\n",
    "    es_tass1_data.append({'text':textt , 'tokens':words , 'sentiment': sents[sentiment] })\n",
    "\n",
    "### Some english tweet data\n",
    "\n",
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/twitter4242.txt\", \"r\", encoding=\"utf-8\",errors='ignore').read().split(\"\\n\")[1:-1]\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "en_twitter_data = list(data)\n",
    "\n",
    "### es2_twitter_data\n",
    "\n",
    "data = open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_dev_complete.txt\", encoding=\"utf-8\").read().split(\"\\n\")[1:-1]\n",
    "data += open(\"../vendors/mtl_girnet/data_prep/data_cm_senti/1600_tweets_test_average_complete.tsv\", encoding=\"utf-8\").read().split(\"\\n\")[1:-2]\n",
    "\n",
    "data = map( lambda x : x.split(\"\\t\") , data )\n",
    "data = map( lambda x :{'sentiment': int(np.sign(int(x[0])-int(x[1]))) , 'tokens': tokenize(x[2]) , 'text': x[2] } , data )\n",
    "\n",
    "es2_twitter_data = list(data)\n",
    "\n",
    "def get_y(data):\n",
    "    from keras.utils import to_categorical\n",
    "    y = []\n",
    "    for row in data:\n",
    "        y.append(int(row['sentiment']))\n",
    "    y = to_categorical(y,num_classes=3)\n",
    "    return y\n",
    "\n",
    "\n",
    "print(\"Code-Mixed: en_es_wssa_data: %d\" % len(en_es_wssa_data))\n",
    "print(\"Spanish: es2_twitter_data: %d\" % len(es2_twitter_data))\n",
    "print(\"Spanish: es_tass1_data: %d\" % len(es_tass1_data))\n",
    "print(\"English: en_twitter_data: %d\" % len(en_twitter_data))\n",
    "# print(\"English: en_sentiment140: %d\" %len(en_sentiment140))\n",
    "en_es_y =  get_y(en_es_wssa_data)\n",
    "en_es_y_train =  get_y(en_es_wssa_data_train)\n",
    "en_es_y_test =  get_y(en_es_wssa_data_test)\n",
    "es_twitter_y = get_y(es2_twitter_data)\n",
    "es_tass_y = get_y(es_tass1_data)\n",
    "en_twitter_y = get_y(en_twitter_data)\n",
    "en_semeval_17_y = get_y(en_semeval_17)\n",
    "# en_sentiment140_y = get_y(en_sentiment140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(en_semeval_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = en_es_wssa_data_train\n",
    "# lang = \"cm\"\n",
    "# for sent in data:\n",
    "#     sentiment_analysis.append(\"\\t\".join([sent['text'], str(sent['sentiment']), lang, \"\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train.txt\", \"w\") as f:\n",
    "#     f.writelines(sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "def get_class_weight(y):\n",
    "    \"\"\"\n",
    "    Used from: https://stackoverflow.com/a/50695814\n",
    "    TODO: check validity and 'balanced' option\n",
    "    :param y: A list of one-hot-encoding labels [[0,0,1,0],[0,0,0,1],..]\n",
    "    :return: class-weights to be used by keras model.fit(.. class_weight=\"\") -> {0:0.52134, 1:1.adas..}\n",
    "    \"\"\"\n",
    "    y_integers = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "\n",
    "def loss_ordinal(y_true, y_pred):\n",
    "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
    "    return (1.0 + weights) * losses.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dim=300 for multilingual BPEmb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "from bpemb import BPEmb\n",
    "multibpemb = BPEmb(lang=\"multi\", vs=1000000, dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = multibpemb.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BPEmb(lang=multi, vs=1000000, dim=300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multibpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "zero_vector = [0 for _ in range(300)]\n",
    "def get_x(data_):\n",
    "    x_  = []\n",
    "    for sent in data_:\n",
    "        pred = list(multibpemb.embed(sent['text']))\n",
    "        if len(pred) >= 32:\n",
    "            pred = pred[:32]\n",
    "        else:\n",
    "            counter = len(pred)\n",
    "            while counter < max_len:\n",
    "                pred.append(zero_vector)\n",
    "                counter = counter + 1\n",
    "        x_.append(pred)\n",
    "    return np.array(x_)\n",
    "en_es_x =  get_x(en_es_wssa_data)\n",
    "es_twitter_x = get_x(es2_twitter_data)\n",
    "es_tass_x = get_x(es_tass1_data)\n",
    "en_twitter_x = get_x(en_twitter_data)\n",
    "en_semeval_17_x = get_x(en_semeval_17)\n",
    "en_es_x_train =  get_x(en_es_wssa_data_train)\n",
    "en_es_x_test =  get_x(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GET Y PADDED TOKENS NUMBER\n",
    "max_len = 32\n",
    "def get_x(data_):\n",
    "    x_  = []\n",
    "    for sent in data_:\n",
    "        pred = list(multibpemb.encode_ids(sent['text']))\n",
    "        if len(pred) >= 32:\n",
    "            pred = pred[:32]\n",
    "        else:\n",
    "            counter = len(pred)\n",
    "            while counter < max_len:\n",
    "                pred.append(0)\n",
    "                counter = counter + 1\n",
    "        x_.append(pred)\n",
    "    return np.array(x_)\n",
    "en_es_x =  get_x(en_es_wssa_data)\n",
    "es_twitter_x = get_x(es2_twitter_data)\n",
    "es_tass_x = get_x(es_tass1_data)\n",
    "en_twitter_x = get_x(en_twitter_data)\n",
    "en_semeval_17_x = get_x(en_semeval_17)\n",
    "en_es_x_train =  get_x(en_es_wssa_data_train)\n",
    "en_es_x_test =  get_x(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed = fasttext.load_model('../vendors/language-models/all_p_fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "zero_vector = [0 for _ in range(100)]\n",
    "def get_x(data_):\n",
    "#     x_  = []\n",
    "#     for sent in data_:\n",
    "#         x_.append(embed.get_sentence_vector(sent['text'].replace(\"\\n\",\" \")))\n",
    "#     return np.array(x_)\n",
    "    x_  = []\n",
    "    for sent in data_:\n",
    "        tokenised = fasttext.tokenize(sent['text'])\n",
    "        sent_vector = []\n",
    "        counter = 0\n",
    "        for token in tokenised:\n",
    "            if counter >= max_len:\n",
    "                break\n",
    "            else:\n",
    "                sent_vector.append(embed[token])\n",
    "                counter = counter + 1\n",
    "        \n",
    "        if counter < max_len:\n",
    "            sent_vector.append(embed['</s>'])\n",
    "            counter = counter + 1\n",
    "                               \n",
    "        while counter < max_len:\n",
    "            sent_vector.append(zero_vector)\n",
    "            counter = counter + 1\n",
    "            \n",
    "        x_.append(sent_vector)\n",
    "        \n",
    "    return np.array(x_)\n",
    "\n",
    "en_es_x =  get_x(en_es_wssa_data)\n",
    "es_twitter_x = get_x(es2_twitter_data)\n",
    "es_tass_x = get_x(es_tass1_data)\n",
    "en_twitter_x = get_x(en_twitter_data)\n",
    "en_semeval_17_x = get_x(en_semeval_17)\n",
    "en_es_x_train =  get_x(en_es_wssa_data_train)\n",
    "en_es_x_test =  get_x(en_es_wssa_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from attention_lstm import AttentionWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(100,)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, dropout=0.3, input_shape=(32, 300), recurrent_dropout=0.3, return_sequences=True)))\n",
    "model.add(AttentionWithContext())\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=loss_ordinal,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-self-attention\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, input_shape=(None, 32, 100))))\n",
    "# model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=loss_ordinal,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(3,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1, input_shape=(32, 300)))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(Conv1D(5,\n",
    "#                  64,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(50, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_f1', mode='max', verbose=1, patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(en_es_x_train, en_es_y_train, epochs=10, shuffle=True, validation_data=(en_es_x_test\n",
    "#                                                                                             ,en_es_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16505 samples, validate on 4127 samples\n",
      "Epoch 1/20\n",
      "16505/16505 [==============================] - 207s 13ms/step - loss: 1.2235 - acc: 0.5872 - f1: 0.4265 - val_loss: 1.3033 - val_acc: 0.5658 - val_f1: 0.5206\n",
      "Epoch 2/20\n",
      "16505/16505 [==============================] - 59s 4ms/step - loss: 1.0800 - acc: 0.6474 - f1: 0.5909 - val_loss: 1.2161 - val_acc: 0.5958 - val_f1: 0.5491\n",
      "Epoch 3/20\n",
      "16505/16505 [==============================] - 58s 4ms/step - loss: 1.0156 - acc: 0.6741 - f1: 0.6304 - val_loss: 1.2169 - val_acc: 0.6089 - val_f1: 0.5821\n",
      "Epoch 4/20\n",
      "16505/16505 [==============================] - 59s 4ms/step - loss: 0.9712 - acc: 0.6899 - f1: 0.6574 - val_loss: 1.1803 - val_acc: 0.6128 - val_f1: 0.5742\n",
      "Epoch 5/20\n",
      "16505/16505 [==============================] - 59s 4ms/step - loss: 0.9356 - acc: 0.7005 - f1: 0.6738 - val_loss: 1.1751 - val_acc: 0.6256 - val_f1: 0.5937\n",
      "Epoch 6/20\n",
      "16505/16505 [==============================] - 58s 4ms/step - loss: 0.8996 - acc: 0.7165 - f1: 0.6937 - val_loss: 1.1901 - val_acc: 0.6179 - val_f1: 0.5918\n",
      "Epoch 7/20\n",
      "16505/16505 [==============================] - 59s 4ms/step - loss: 0.8676 - acc: 0.7251 - f1: 0.7071 - val_loss: 1.2249 - val_acc: 0.6118 - val_f1: 0.5921\n",
      "Epoch 8/20\n",
      "16505/16505 [==============================] - 58s 3ms/step - loss: 0.8332 - acc: 0.7380 - f1: 0.7216 - val_loss: 1.1799 - val_acc: 0.6266 - val_f1: 0.5969\n",
      "Epoch 9/20\n",
      "16505/16505 [==============================] - 59s 4ms/step - loss: 0.7964 - acc: 0.7526 - f1: 0.7360 - val_loss: 1.1973 - val_acc: 0.6162 - val_f1: 0.5948\n",
      "Epoch 10/20\n",
      "16505/16505 [==============================] - 57s 3ms/step - loss: 0.7674 - acc: 0.7594 - f1: 0.7492 - val_loss: 1.2435 - val_acc: 0.6220 - val_f1: 0.6080\n",
      "Epoch 11/20\n",
      "16505/16505 [==============================] - 50s 3ms/step - loss: 0.7395 - acc: 0.7715 - f1: 0.7611 - val_loss: 1.2920 - val_acc: 0.6155 - val_f1: 0.6070\n",
      "Epoch 12/20\n",
      "16505/16505 [==============================] - 53s 3ms/step - loss: 0.7141 - acc: 0.7788 - f1: 0.7712 - val_loss: 1.2991 - val_acc: 0.6179 - val_f1: 0.6067\n",
      "Epoch 13/20\n",
      "16505/16505 [==============================] - 69s 4ms/step - loss: 0.6843 - acc: 0.7896 - f1: 0.7820 - val_loss: 1.2764 - val_acc: 0.6189 - val_f1: 0.6060\n",
      "Epoch 14/20\n",
      "16505/16505 [==============================] - 283s 17ms/step - loss: 0.6525 - acc: 0.7989 - f1: 0.7929 - val_loss: 1.3820 - val_acc: 0.6189 - val_f1: 0.6131\n",
      "Epoch 15/20\n",
      "16505/16505 [==============================] - 284s 17ms/step - loss: 0.6324 - acc: 0.8050 - f1: 0.7987 - val_loss: 1.3387 - val_acc: 0.6150 - val_f1: 0.6070\n",
      "Epoch 16/20\n",
      "16505/16505 [==============================] - 285s 17ms/step - loss: 0.6103 - acc: 0.8104 - f1: 0.8056 - val_loss: 1.3926 - val_acc: 0.6203 - val_f1: 0.6108\n",
      "Epoch 17/20\n",
      "16505/16505 [==============================] - 286s 17ms/step - loss: 0.5957 - acc: 0.8199 - f1: 0.8150 - val_loss: 1.4707 - val_acc: 0.6130 - val_f1: 0.6074\n",
      "Epoch 18/20\n",
      "16505/16505 [==============================] - 285s 17ms/step - loss: 0.5736 - acc: 0.8278 - f1: 0.8223 - val_loss: 1.4612 - val_acc: 0.6198 - val_f1: 0.6141\n",
      "Epoch 19/20\n",
      "16505/16505 [==============================] - 287s 17ms/step - loss: 0.5427 - acc: 0.8346 - f1: 0.8324 - val_loss: 1.5162 - val_acc: 0.6242 - val_f1: 0.6186\n",
      "Epoch 20/20\n",
      "16505/16505 [==============================] - 286s 17ms/step - loss: 0.5363 - acc: 0.8394 - f1: 0.8363 - val_loss: 1.5130 - val_acc: 0.6067 - val_f1: 0.6018\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(en_semeval_17_x, en_semeval_17_y, epochs=20, initial_epoch=0, validation_split=0.2, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5036796031923902, 0.5236541600639629, 0.5194763644290009]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(en_es_x_test, en_es_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5773 samples, validate on 1444 samples\n",
      "Epoch 21/35\n",
      "5773/5773 [==============================] - 99s 17ms/step - loss: 1.5766 - acc: 0.4731 - f1: 0.3279 - val_loss: 1.2728 - val_acc: 0.5845 - val_f1: 0.4095\n",
      "Epoch 22/35\n",
      "5773/5773 [==============================] - 99s 17ms/step - loss: 1.2965 - acc: 0.5597 - f1: 0.4235 - val_loss: 1.2187 - val_acc: 0.6066 - val_f1: 0.4645\n",
      "Epoch 23/35\n",
      "5773/5773 [==============================] - 100s 17ms/step - loss: 1.2240 - acc: 0.5948 - f1: 0.4916 - val_loss: 1.1890 - val_acc: 0.6115 - val_f1: 0.4895\n",
      "Epoch 24/35\n",
      "5773/5773 [==============================] - 100s 17ms/step - loss: 1.1629 - acc: 0.6248 - f1: 0.5262 - val_loss: 1.1804 - val_acc: 0.6157 - val_f1: 0.5274\n",
      "Epoch 25/35\n",
      "5773/5773 [==============================] - 100s 17ms/step - loss: 1.1160 - acc: 0.6409 - f1: 0.5725 - val_loss: 1.1644 - val_acc: 0.6288 - val_f1: 0.5711\n",
      "Epoch 26/35\n",
      "5773/5773 [==============================] - 101s 18ms/step - loss: 1.0734 - acc: 0.6584 - f1: 0.5998 - val_loss: 1.1762 - val_acc: 0.6337 - val_f1: 0.5858\n",
      "Epoch 27/35\n",
      "5248/5773 [==========================>...] - ETA: 8s - loss: 1.0275 - acc: 0.6766 - f1: 0.6293"
     ]
    }
   ],
   "source": [
    "history = model.fit(es_tass_x, es_tass_y, epochs=35, initial_epoch=20, validation_split=0.2, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(en_es_x_test, en_es_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(es_twitter_x, es_twitter_y, epochs=40, initial_epoch=35, validation_split=0.2, shuffle=True,  callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(en_es_x_test, en_es_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([en_semeval_17_x, en_twitter_x, es_tass_x, es_twitter_x])\n",
    "y = np.concatenate([en_semeval_17_y, en_twitter_y, es_tass_y, es_twitter_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, y, epochs=60, initial_epoch=40,validation_data=(en_es_x_train,en_es_y_train), shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(en_es_x_test, en_es_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(en_es_x_train, en_es_y_train, epochs=80, initial_epoch=60, validation_data=(en_es_x_test, en_es_y_test), shuffle=True,  callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(en_es_x_test, en_es_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from attention_lstm import AttentionWithContext\n",
    "from keras.callbacks import *\n",
    "# from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GiretTwoCell(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, cell_1 , cell_2 , nHidden , **kwargs):\n",
    "        self.cell_1 = cell_1\n",
    "        self.cell_2 = cell_2\n",
    "        self.nHidden = nHidden\n",
    "        self.state_size = [nHidden,nHidden]\n",
    "        super(GiretTwoCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        input_shape_n = ( input_shape[0] , input_shape[1]- 2 )\n",
    "#         print \"pp\", input_shape_n\n",
    "        \n",
    "#         self.cell_1.build(input_shape_n)\n",
    "#         self.cell_2.build(input_shape_n)\n",
    "        \n",
    "        self._trainable_weights += ( self.cell_1.trainable_weights )\n",
    "        self._trainable_weights += ( self.cell_2.trainable_weights )\n",
    "        \n",
    "        self._non_trainable_weights += (  self.cell_1.non_trainable_weights )\n",
    "        self._non_trainable_weights += (  self.cell_2.non_trainable_weights )\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        \n",
    "        nHidden = self.nHidden\n",
    "        \n",
    "        gate_val_1 = inputs[ : , 0:1]\n",
    "        gate_val_2 = inputs[ : , 1:2]\n",
    "        \n",
    "        inputs  = inputs[ : , 2: ]\n",
    "                \n",
    "        gate_val_1 = K.repeat_elements(gate_val_1 , nHidden , -1 ) # shape # bs , hidden\n",
    "        gate_val_2 = K.repeat_elements(gate_val_2 , nHidden , -1 ) # shape # bs , hidden\n",
    "        \n",
    "        _ , [h1 , c1 ]  = self.cell_1.call( inputs , states )\n",
    "        _ , [h2 , c2 ]  = self.cell_2.call( inputs , states )\n",
    "        \n",
    "        h = gate_val_1*h1 + gate_val_2*h2  + (1 - gate_val_1 -  gate_val_2 )*states[0]\n",
    "        c = gate_val_1*c1 + gate_val_2*c2  + (1 - gate_val_1 -  gate_val_2 )*states[1]\n",
    "        \n",
    "        return h, [h , c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 100\n",
    "dims = 300\n",
    "\n",
    "rnn_en = LSTM(hidden, name='en_lstm', recurrent_dropout=0.3, dropout=0.3)\n",
    "rnn_hi = LSTM(hidden, name='es_lstm', recurrent_dropout=0.3, dropout=0.3)\n",
    "\n",
    "       \n",
    "# en\n",
    "inp_en = Input(shape=(32, dims))\n",
    "rnn_en_ = rnn_en(inp_en)\n",
    "x = Dropout(0.3)(rnn_en_)\n",
    "out_en = Dense(3, activation='softmax', name='en')(x)\n",
    "\n",
    "\n",
    "# es\n",
    "inp_hi = Input(shape=(32, dims))\n",
    "rnn_hi_ = rnn_hi(inp_hi)\n",
    "x = Dropout(0.3)(rnn_hi_)\n",
    "out_hi = Dense(3, activation='softmax', name='es')(x)\n",
    "\n",
    "\n",
    "cell_combined = GiretTwoCell(rnn_en.cell , rnn_hi.cell , hidden)\n",
    "\n",
    "        \n",
    "inp_enhi = Input(shape=(32, dims))\n",
    "x = inp_enhi\n",
    "x_att = x\n",
    "x_att = Bidirectional(LSTM(32 , return_sequences=True, recurrent_dropout=0.3, dropout=0.3))( x )\n",
    "bider_h = x_att \n",
    "x_att = Dropout(0.3)(x_att)\n",
    "x_att = TimeDistributed(Dense(3, activation='softmax') )(x_att)\n",
    "x_att = Lambda(lambda x : x[... , 1: ])(x_att)\n",
    "\n",
    "x = Concatenate(-1)([x_att , x ])\n",
    "\n",
    "x =  RNN(cell_combined, name='damn')(x)\n",
    "# x = AttentionWithContext()(x)\n",
    "out_enhi = Dense(3, activation='softmax', name='cm')(x)\n",
    "        \n",
    "model = Model( [inp_en , inp_hi , inp_enhi  ] , [ out_en , out_hi , out_enhi ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 100\n",
    "numwords = weight_matrix.shape[0]\n",
    "hidden_emd_dim = 300\n",
    "\n",
    "\n",
    "embed = Embedding(numwords, hidden_emd_dim, weights=[weight_matrix], mask_zero=True)\n",
    "# conv1 = Conv1D(64, 3, activation='relu', padding='valid',strides=1)\n",
    "# pool1 = MaxPooling1D(2)\n",
    "# conv2 = Conv1D(64, 5, activation='relu', padding='valid', strides=1)\n",
    "# pool2 = MaxPooling1D(2)\n",
    "# conv3 = Conv1D(128, 5, activation='relu', padding='valid', strides=1)\n",
    "# pool3 = MaxPooling1D(2)\n",
    "# conv3 = Conv1D(128, 5, activation='relu', padding='valid',strides=1)\n",
    "# pool3 = MaxPooling1D(35)  # global max pooling\n",
    "\n",
    "\n",
    "rnn_en = LSTM(hidden, name='en_lstm', recurrent_dropout=0.3, dropout=0.3)\n",
    "rnn_hi = LSTM(hidden, name=\"hi_lstm\", recurrent_dropout=0.3, dropout=0.3)\n",
    "       \n",
    "# en\n",
    "inp_en = Input((None, ))\n",
    "x = embed(inp_en)\n",
    "# x = conv1(x)\n",
    "# x = pool1(x)\n",
    "# x = conv2(x)\n",
    "# x = pool2(x)\n",
    "# x = conv3(x)\n",
    "# x = pool3(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = rnn_en(x)\n",
    "out_en = Dense(3, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# es\n",
    "inp_hi = Input((None, ))\n",
    "x = embed(inp_hi)\n",
    "# x = conv1(x)\n",
    "# x = pool1(x)\n",
    "# x = conv2(x)\n",
    "# x = pool2(x)\n",
    "# x = conv3(x)\n",
    "# x = pool3(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = rnn_hi( x )\n",
    "out_hi = Dense(3, activation='softmax')(x)\n",
    "\n",
    "\n",
    "cell_combined = GiretTwoCell(rnn_hi.cell , rnn_en.cell , hidden)\n",
    "\n",
    "inp_enhi = Input((None, ))\n",
    "x = embed(inp_enhi)\n",
    "# x = conv1(x)\n",
    "# x = pool1(x)\n",
    "# x = conv2(x)\n",
    "# x = pool2(x)\n",
    "# x = conv3(x)\n",
    "# x = pool3(x)\n",
    "x_att = x\n",
    "x_att = Bidirectional(LSTM(32 , return_sequences=True, recurrent_dropout=0.3, dropout=0.3))( x )\n",
    "bider_h = x_att \n",
    "x_att = Dropout(0.3)(x_att)\n",
    "x_att = TimeDistributed(Dense(3, activation='softmax') )(x_att)\n",
    "x_att = Lambda(lambda x : x[... , 1: ])(x_att)\n",
    "\n",
    "x = Concatenate(-1)([x_att , x ])\n",
    "\n",
    "x =  RNN(cell_combined, name='damn')(x)\n",
    "# x = AttentionWithContext()(x)\n",
    "out_enhi = Dense(3, activation='softmax', name='cm')(x)\n",
    "        \n",
    "model = Model( [inp_hi , inp_en , inp_enhi  ] , [ out_hi , out_en , out_enhi ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_cm_f1', mode='max', verbose=1, patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_66 (InputLayer)           (None, 32, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 32, 64)       85248       input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 32, 64)       0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, 32, 3)        195         dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_64 (InputLayer)           (None, 32, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_65 (InputLayer)           (None, 32, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 32, 2)        0           time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "en_lstm (LSTM)                  (None, 100)          160400      input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "es_lstm (LSTM)                  (None, 100)          160400      input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 32, 302)      0           lambda_22[0][0]                  \n",
      "                                                                 input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 100)          0           en_lstm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 100)          0           es_lstm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "damn (RNN)                      (None, 100)          0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "en (Dense)                      (None, 3)            303         dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "es (Dense)                      (None, 3)            303         dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cm (Dense)                      (None, 3)            303         damn[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 407,152\n",
      "Trainable params: 407,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_x = np.concatenate([en_twitter_x, en_semeval_17_x])\n",
    "en_y = np.concatenate([en_twitter_y, en_semeval_17_y])\n",
    "es_x = np.concatenate([es_twitter_x, es_tass_x])\n",
    "es_y = np.concatenate([es_twitter_y, es_tass_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_x = np.concatenate([en_semeval_17_x, en_twitter_x, es_tass_x, es_twitter_x, en_es_x_train])\n",
    "# all_y = np.concatenate([en_semeval_17_y, en_twitter_y, es_tass_y, es_twitter_y, en_es_y_train])\n",
    "all_x = np.concatenate([en_semeval_17_x, es_tass_x, es_twitter_x])\n",
    "all_y = np.concatenate([en_semeval_17_y, es_tass_y, es_twitter_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_generator(batch_size=4, lang=\"cm\"):\n",
    "    b = batch_size\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "#         n3 = np.random.randint(0, en_es_x_train.shape[0] - batch_size, batch_size)\n",
    "\n",
    "\n",
    "        if lang is \"cm\": \n",
    "            n1 = np.random.randint(0, en_x.shape[0] - batch_size, batch_size)\n",
    "            n2 = np.random.randint(0, es_x.shape[0] - batch_size, batch_size)\n",
    "            p = random.random()\n",
    "            if p < 0.3:\n",
    "                p = random.random()\n",
    "                if p < 0.5:\n",
    "                    n3 = np.random.randint(0, en_x.shape[0] - batch_size, batch_size)\n",
    "                    x = [en_x[n1,:], es_x[n2,:], en_x[n3,:]]\n",
    "                    y = [en_y[n1,:], es_y[n2,:], en_y[n3,:]]\n",
    "                else:\n",
    "                    n3 = np.random.randint(0, es_x.shape[0] - batch_size, batch_size)\n",
    "                    x = [en_x[n1,:], es_x[n2,:], es_x[n3,:]]\n",
    "                    y = [en_y[n1,:], es_y[n2,:], es_y[n3,:]]\n",
    "            else:\n",
    "                n3 = np.random.randint(0, en_es_x_train.shape[0] - batch_size, batch_size)\n",
    "                x = [en_x[n1,:], es_x[n2,:], en_es_x_train[n3,:]]\n",
    "                y = [en_y[n1,:], es_y[n2,:], en_es_y_train[n3,:]]\n",
    "        elif lang is \"en\":\n",
    "            n3 = np.random.randint(0, en_x.shape[0] - batch_size, batch_size)\n",
    "            x = [en_x[n3,:], en_x[n3,:], en_x[n3,:]]\n",
    "            y = [en_y[n3,:], en_y[n3,:], en_y[n3,:]]\n",
    "        elif lang is \"es\":\n",
    "            n3 = np.random.randint(0, es_x.shape[0] - batch_size, batch_size)\n",
    "            x = [es_x[n3,:], es_x[n3,:], es_x[n3,:]]\n",
    "            y = [es_y[n3,:], es_y[n3,:], es_y[n3,:]]\n",
    "        elif lang is \"unsup\":\n",
    "            n1 = np.random.randint(0, en_x.shape[0] - batch_size, batch_size)\n",
    "            n2 = np.random.randint(0, es_x.shape[0] - batch_size, batch_size)\n",
    "            n3 = np.random.randint(0, all_x.shape[0] - batch_size, batch_size)\n",
    "            x = [en_x[n1,:], es_x[n2,:], all_x[n3,:]]\n",
    "            y = [en_y[n1,:], es_y[n2,:], all_y[n3,:]]\n",
    "            \n",
    "                 \n",
    "        \n",
    "#         x = [en_x[n1,:], es_x[n2,:], all_x[n3,:]]\n",
    "#         y = [en_y[n1,:], es_y[n2,:], all_y[n3,:]]\n",
    "#         x = [ en_es_x_train[n3,:],  en_es_x_train[n3,:], en_es_x_train[n3,:]]\n",
    "#         y = [en_es_y_train[n3,:], en_es_y_train[n3,:], en_es_y_train[n3,:]]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 99s 494ms/step - loss: 2.9835 - en_loss: 1.0056 - es_loss: 1.0017 - cm_loss: 0.9762 - en_f1: 0.3275 - es_f1: 0.3266 - cm_f1: 0.3567 - val_loss: 3.2282 - val_en_loss: 1.0795 - val_es_loss: 1.0830 - val_cm_loss: 1.0657 - val_en_f1: 0.0321 - val_es_f1: 0.1118 - val_cm_f1: 0.1290\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 2.7458 - en_loss: 0.9441 - es_loss: 0.9313 - cm_loss: 0.8704 - en_f1: 0.4483 - es_f1: 0.4659 - cm_f1: 0.5291 - val_loss: 3.1572 - val_en_loss: 1.0790 - val_es_loss: 1.0652 - val_cm_loss: 1.0130 - val_en_f1: 0.3614 - val_es_f1: 0.3692 - val_cm_f1: 0.2986\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 70s 349ms/step - loss: 2.6826 - en_loss: 0.9231 - es_loss: 0.9087 - cm_loss: 0.8508 - en_f1: 0.4839 - es_f1: 0.4845 - cm_f1: 0.5464 - val_loss: 3.1598 - val_en_loss: 1.0586 - val_es_loss: 1.0855 - val_cm_loss: 1.0158 - val_en_f1: 0.3208 - val_es_f1: 0.4056 - val_cm_f1: 0.3968\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 71s 355ms/step - loss: 2.6116 - en_loss: 0.8988 - es_loss: 0.8838 - cm_loss: 0.8290 - en_f1: 0.4923 - es_f1: 0.5187 - cm_f1: 0.5662 - val_loss: 3.0497 - val_en_loss: 1.0420 - val_es_loss: 1.0165 - val_cm_loss: 0.9913 - val_en_f1: 0.3004 - val_es_f1: 0.3617 - val_cm_f1: 0.4051\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 71s 355ms/step - loss: 2.4996 - en_loss: 0.8600 - es_loss: 0.8452 - cm_loss: 0.7944 - en_f1: 0.5414 - es_f1: 0.5625 - cm_f1: 0.6024 - val_loss: 3.1131 - val_en_loss: 1.0633 - val_es_loss: 1.0434 - val_cm_loss: 1.0064 - val_en_f1: 0.3817 - val_es_f1: 0.4203 - val_cm_f1: 0.4522\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 70s 352ms/step - loss: 2.4920 - en_loss: 0.8580 - es_loss: 0.8442 - cm_loss: 0.7897 - en_f1: 0.5456 - es_f1: 0.5575 - cm_f1: 0.6079 - val_loss: 3.0513 - val_en_loss: 1.0367 - val_es_loss: 1.0219 - val_cm_loss: 0.9927 - val_en_f1: 0.3369 - val_es_f1: 0.3941 - val_cm_f1: 0.4342\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 70s 348ms/step - loss: 2.4592 - en_loss: 0.8407 - es_loss: 0.8381 - cm_loss: 0.7804 - en_f1: 0.5516 - es_f1: 0.5626 - cm_f1: 0.6059 - val_loss: 3.0629 - val_en_loss: 1.0509 - val_es_loss: 1.0175 - val_cm_loss: 0.9944 - val_en_f1: 0.3895 - val_es_f1: 0.4255 - val_cm_f1: 0.4597\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 2.4508 - en_loss: 0.8382 - es_loss: 0.8313 - cm_loss: 0.7812 - en_f1: 0.5655 - es_f1: 0.5772 - cm_f1: 0.6176 - val_loss: 3.0830 - val_en_loss: 1.0687 - val_es_loss: 1.0273 - val_cm_loss: 0.9870 - val_en_f1: 0.3856 - val_es_f1: 0.3980 - val_cm_f1: 0.4549\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 2.4219 - en_loss: 0.8345 - es_loss: 0.8202 - cm_loss: 0.7672 - en_f1: 0.5706 - es_f1: 0.5787 - cm_f1: 0.6234 - val_loss: 3.1562 - val_en_loss: 1.0714 - val_es_loss: 1.0600 - val_cm_loss: 1.0247 - val_en_f1: 0.4318 - val_es_f1: 0.4586 - val_cm_f1: 0.4889\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 2.3601 - en_loss: 0.8131 - es_loss: 0.7965 - cm_loss: 0.7506 - en_f1: 0.5835 - es_f1: 0.6066 - cm_f1: 0.6326 - val_loss: 3.1468 - val_en_loss: 1.0851 - val_es_loss: 1.0594 - val_cm_loss: 1.0024 - val_en_f1: 0.4470 - val_es_f1: 0.4507 - val_cm_f1: 0.4822\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 2.3381 - en_loss: 0.8039 - es_loss: 0.7933 - cm_loss: 0.7408 - en_f1: 0.6007 - es_f1: 0.6066 - cm_f1: 0.6433 - val_loss: 3.2106 - val_en_loss: 1.0716 - val_es_loss: 1.1166 - val_cm_loss: 1.0224 - val_en_f1: 0.4711 - val_es_f1: 0.4758 - val_cm_f1: 0.5045\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 2.3227 - en_loss: 0.7971 - es_loss: 0.7864 - cm_loss: 0.7392 - en_f1: 0.6028 - es_f1: 0.6092 - cm_f1: 0.6425 - val_loss: 3.2578 - val_en_loss: 1.1174 - val_es_loss: 1.0839 - val_cm_loss: 1.0565 - val_en_f1: 0.4382 - val_es_f1: 0.3973 - val_cm_f1: 0.4692\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 2.2997 - en_loss: 0.7907 - es_loss: 0.7754 - cm_loss: 0.7336 - en_f1: 0.6033 - es_f1: 0.6100 - cm_f1: 0.6441 - val_loss: 3.2414 - val_en_loss: 1.0692 - val_es_loss: 1.1540 - val_cm_loss: 1.0182 - val_en_f1: 0.4552 - val_es_f1: 0.4717 - val_cm_f1: 0.4964\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.2837 - en_loss: 0.7852 - es_loss: 0.7729 - cm_loss: 0.7257 - en_f1: 0.6083 - es_f1: 0.6213 - cm_f1: 0.6561 - val_loss: 3.1932 - val_en_loss: 1.0663 - val_es_loss: 1.0827 - val_cm_loss: 1.0442 - val_en_f1: 0.4643 - val_es_f1: 0.4783 - val_cm_f1: 0.4870\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 2.2646 - en_loss: 0.7777 - es_loss: 0.7650 - cm_loss: 0.7219 - en_f1: 0.6157 - es_f1: 0.6268 - cm_f1: 0.6549 - val_loss: 3.1128 - val_en_loss: 1.0367 - val_es_loss: 1.0465 - val_cm_loss: 1.0296 - val_en_f1: 0.4401 - val_es_f1: 0.4485 - val_cm_f1: 0.4835\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 2.2613 - en_loss: 0.7719 - es_loss: 0.7684 - cm_loss: 0.7211 - en_f1: 0.6195 - es_f1: 0.6306 - cm_f1: 0.6575 - val_loss: 3.3130 - val_en_loss: 1.1398 - val_es_loss: 1.1114 - val_cm_loss: 1.0618 - val_en_f1: 0.4734 - val_es_f1: 0.4926 - val_cm_f1: 0.5064\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.2043 - en_loss: 0.7526 - es_loss: 0.7463 - cm_loss: 0.7055 - en_f1: 0.6358 - es_f1: 0.6394 - cm_f1: 0.6686 - val_loss: 3.2170 - val_en_loss: 1.0614 - val_es_loss: 1.0915 - val_cm_loss: 1.0641 - val_en_f1: 0.4477 - val_es_f1: 0.4762 - val_cm_f1: 0.4950\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.1358 - en_loss: 0.7317 - es_loss: 0.7281 - cm_loss: 0.6760 - en_f1: 0.6552 - es_f1: 0.6608 - cm_f1: 0.6845 - val_loss: 3.3162 - val_en_loss: 1.1007 - val_es_loss: 1.0876 - val_cm_loss: 1.1280 - val_en_f1: 0.4841 - val_es_f1: 0.4837 - val_cm_f1: 0.5159\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 2.1540 - en_loss: 0.7429 - es_loss: 0.7296 - cm_loss: 0.6815 - en_f1: 0.6452 - es_f1: 0.6586 - cm_f1: 0.6907 - val_loss: 3.1450 - val_en_loss: 1.0568 - val_es_loss: 1.0530 - val_cm_loss: 1.0353 - val_en_f1: 0.4853 - val_es_f1: 0.4629 - val_cm_f1: 0.4986\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 2.1702 - en_loss: 0.7458 - es_loss: 0.7373 - cm_loss: 0.6871 - en_f1: 0.6387 - es_f1: 0.6488 - cm_f1: 0.6817 - val_loss: 3.3339 - val_en_loss: 1.1216 - val_es_loss: 1.1420 - val_cm_loss: 1.0703 - val_en_f1: 0.4833 - val_es_f1: 0.4928 - val_cm_f1: 0.5023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfee3a3240>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = train_generator(32, lang=\"en\")\n",
    "model.fit_generator(\n",
    "    generator=gen,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=20,\n",
    "    initial_epoch=0,\n",
    "    validation_data=([en_es_x_train,en_es_x_train,en_es_x_train],[en_es_y_train,en_es_y_train,en_es_y_train]),\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.199548551736804,\n",
       " 1.0782622342980899,\n",
       " 1.119319643515358,\n",
       " 1.0019666784933692,\n",
       " 0.4979645117849938,\n",
       " 0.5129714421502529,\n",
       " 0.5279189461012649]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([en_es_x_test,en_es_x_test,en_es_x_test],[en_es_y_test,en_es_y_test,en_es_y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 2.7181 - en_loss: 0.9273 - es_loss: 0.9161 - cm_loss: 0.8746 - en_f1: 0.4807 - es_f1: 0.4805 - cm_f1: 0.5307 - val_loss: 2.9684 - val_en_loss: 0.9849 - val_es_loss: 1.0129 - val_cm_loss: 0.9706 - val_en_f1: 0.3642 - val_es_f1: 0.3680 - val_cm_f1: 0.4227\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 2.6586 - en_loss: 0.9075 - es_loss: 0.9001 - cm_loss: 0.8510 - en_f1: 0.5004 - es_f1: 0.5064 - cm_f1: 0.5464 - val_loss: 3.0103 - val_en_loss: 1.0009 - val_es_loss: 1.0308 - val_cm_loss: 0.9785 - val_en_f1: 0.3912 - val_es_f1: 0.3855 - val_cm_f1: 0.4332\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 2.5710 - en_loss: 0.8764 - es_loss: 0.8716 - cm_loss: 0.8231 - en_f1: 0.5302 - es_f1: 0.5338 - cm_f1: 0.5800 - val_loss: 2.9658 - val_en_loss: 1.0016 - val_es_loss: 1.0068 - val_cm_loss: 0.9574 - val_en_f1: 0.4110 - val_es_f1: 0.4077 - val_cm_f1: 0.4531\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 61s 305ms/step - loss: 2.5215 - en_loss: 0.8589 - es_loss: 0.8553 - cm_loss: 0.8073 - en_f1: 0.5526 - es_f1: 0.5530 - cm_f1: 0.5909 - val_loss: 2.9454 - val_en_loss: 0.9952 - val_es_loss: 0.9868 - val_cm_loss: 0.9634 - val_en_f1: 0.4230 - val_es_f1: 0.3968 - val_cm_f1: 0.4465\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 2.4690 - en_loss: 0.8448 - es_loss: 0.8343 - cm_loss: 0.7900 - en_f1: 0.5557 - es_f1: 0.5743 - cm_f1: 0.6078 - val_loss: 2.9935 - val_en_loss: 1.0004 - val_es_loss: 1.0110 - val_cm_loss: 0.9821 - val_en_f1: 0.4414 - val_es_f1: 0.4146 - val_cm_f1: 0.4538\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 2.4536 - en_loss: 0.8351 - es_loss: 0.8291 - cm_loss: 0.7893 - en_f1: 0.5740 - es_f1: 0.5705 - cm_f1: 0.6032 - val_loss: 2.9597 - val_en_loss: 0.9860 - val_es_loss: 0.9982 - val_cm_loss: 0.9756 - val_en_f1: 0.4198 - val_es_f1: 0.4124 - val_cm_f1: 0.4507\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 2.3740 - en_loss: 0.8098 - es_loss: 0.8043 - cm_loss: 0.7599 - en_f1: 0.6016 - es_f1: 0.5981 - cm_f1: 0.6284 - val_loss: 3.0015 - val_en_loss: 1.0036 - val_es_loss: 1.0085 - val_cm_loss: 0.9894 - val_en_f1: 0.4477 - val_es_f1: 0.4299 - val_cm_f1: 0.4456\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 68s 338ms/step - loss: 2.3544 - en_loss: 0.8064 - es_loss: 0.8028 - cm_loss: 0.7452 - en_f1: 0.5968 - es_f1: 0.5953 - cm_f1: 0.6336 - val_loss: 3.0067 - val_en_loss: 1.0024 - val_es_loss: 1.0090 - val_cm_loss: 0.9953 - val_en_f1: 0.4331 - val_es_f1: 0.3932 - val_cm_f1: 0.4539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfee3a32e8>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = train_generator(32, lang=\"es\")\n",
    "model.fit_generator(\n",
    "    generator=gen,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=30,\n",
    "    initial_epoch=20,\n",
    "    validation_data=([en_es_x_train,en_es_x_train,en_es_x_train],[en_es_y_train,en_es_y_train,en_es_y_train]),\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8801771301235304,\n",
       " 0.9730363803236558,\n",
       " 0.9758537548790165,\n",
       " 0.9312869851001907,\n",
       " 0.43307197920455437,\n",
       " 0.414544276856676,\n",
       " 0.47206871213959634]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([en_es_x_test,en_es_x_test,en_es_x_test],[en_es_y_test,en_es_y_test,en_es_y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 2.3080 - en_loss: 0.7980 - es_loss: 0.7802 - cm_loss: 0.7299 - en_f1: 0.6096 - es_f1: 0.6164 - cm_f1: 0.6543 - val_loss: 2.9139 - val_en_loss: 0.9517 - val_es_loss: 0.9950 - val_cm_loss: 0.9672 - val_en_f1: 0.4389 - val_es_f1: 0.4202 - val_cm_f1: 0.4625\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 2.2438 - en_loss: 0.7618 - es_loss: 0.7629 - cm_loss: 0.7192 - en_f1: 0.6205 - es_f1: 0.6317 - cm_f1: 0.6543 - val_loss: 2.9691 - val_en_loss: 0.9527 - val_es_loss: 1.0303 - val_cm_loss: 0.9861 - val_en_f1: 0.4531 - val_es_f1: 0.3988 - val_cm_f1: 0.4570\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 2.2081 - en_loss: 0.7464 - es_loss: 0.7489 - cm_loss: 0.7128 - en_f1: 0.6434 - es_f1: 0.6431 - cm_f1: 0.6643 - val_loss: 2.8913 - val_en_loss: 0.9556 - val_es_loss: 0.9767 - val_cm_loss: 0.9590 - val_en_f1: 0.4462 - val_es_f1: 0.4557 - val_cm_f1: 0.4862\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 2.1647 - en_loss: 0.7437 - es_loss: 0.7457 - cm_loss: 0.6753 - en_f1: 0.6457 - es_f1: 0.6424 - cm_f1: 0.6847 - val_loss: 2.9072 - val_en_loss: 0.9406 - val_es_loss: 0.9964 - val_cm_loss: 0.9702 - val_en_f1: 0.4491 - val_es_f1: 0.4564 - val_cm_f1: 0.4788\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 64s 322ms/step - loss: 2.1441 - en_loss: 0.7366 - es_loss: 0.7274 - cm_loss: 0.6800 - en_f1: 0.6393 - es_f1: 0.6613 - cm_f1: 0.6806 - val_loss: 2.9040 - val_en_loss: 0.9319 - val_es_loss: 1.0058 - val_cm_loss: 0.9662 - val_en_f1: 0.4884 - val_es_f1: 0.4514 - val_cm_f1: 0.4756\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 2.1057 - en_loss: 0.7261 - es_loss: 0.7125 - cm_loss: 0.6671 - en_f1: 0.6609 - es_f1: 0.6691 - cm_f1: 0.6937 - val_loss: 2.9259 - val_en_loss: 0.9543 - val_es_loss: 1.0099 - val_cm_loss: 0.9618 - val_en_f1: 0.4654 - val_es_f1: 0.4784 - val_cm_f1: 0.4957\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 2.1030 - en_loss: 0.7281 - es_loss: 0.6964 - cm_loss: 0.6784 - en_f1: 0.6514 - es_f1: 0.6764 - cm_f1: 0.6903 - val_loss: 2.9682 - val_en_loss: 0.9493 - val_es_loss: 1.0344 - val_cm_loss: 0.9845 - val_en_f1: 0.4925 - val_es_f1: 0.4706 - val_cm_f1: 0.4728\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 2.0648 - en_loss: 0.7192 - es_loss: 0.6778 - cm_loss: 0.6679 - en_f1: 0.6577 - es_f1: 0.6934 - cm_f1: 0.6847 - val_loss: 3.0030 - val_en_loss: 0.9540 - val_es_loss: 1.0434 - val_cm_loss: 1.0056 - val_en_f1: 0.4795 - val_es_f1: 0.4798 - val_cm_f1: 0.4934\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 2.0361 - en_loss: 0.7072 - es_loss: 0.6682 - cm_loss: 0.6608 - en_f1: 0.6714 - es_f1: 0.6964 - cm_f1: 0.7033 - val_loss: 2.9966 - val_en_loss: 0.9567 - val_es_loss: 1.0413 - val_cm_loss: 0.9986 - val_en_f1: 0.4765 - val_es_f1: 0.5061 - val_cm_f1: 0.4868\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 2.0189 - en_loss: 0.7011 - es_loss: 0.6671 - cm_loss: 0.6508 - en_f1: 0.6756 - es_f1: 0.6926 - cm_f1: 0.6981 - val_loss: 2.9975 - val_en_loss: 0.9711 - val_es_loss: 1.0226 - val_cm_loss: 1.0038 - val_en_f1: 0.4741 - val_es_f1: 0.4752 - val_cm_f1: 0.4858\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 1.9756 - en_loss: 0.6897 - es_loss: 0.6362 - cm_loss: 0.6497 - en_f1: 0.6830 - es_f1: 0.7158 - cm_f1: 0.7022 - val_loss: 3.0423 - val_en_loss: 0.9790 - val_es_loss: 1.0587 - val_cm_loss: 1.0047 - val_en_f1: 0.4898 - val_es_f1: 0.4886 - val_cm_f1: 0.4952\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00041: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfe64a9b00>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = train_generator(32, lang=\"unsup\")\n",
    "model.fit_generator(\n",
    "    generator=gen,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=50,\n",
    "    initial_epoch=30,\n",
    "    validation_data=([en_es_x_train,en_es_x_train,en_es_x_train],[en_es_y_train,en_es_y_train,en_es_y_train]),\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8389667408695813,\n",
       " 0.9418664416131148,\n",
       " 0.98001595840563,\n",
       " 0.9170843568945009,\n",
       " 0.5008024851416297,\n",
       " 0.49733431868218675,\n",
       " 0.5216923524271606]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([en_es_x_test,en_es_x_test,en_es_x_test],[en_es_y_test,en_es_y_test,en_es_y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 1.4262 - en_loss: 0.6162 - es_loss: 0.4875 - cm_loss: 0.3225 - en_f1: 0.7276 - es_f1: 0.7991 - cm_f1: 0.8785 - val_loss: 3.1155 - val_en_loss: 0.9687 - val_es_loss: 1.0265 - val_cm_loss: 1.1203 - val_en_f1: 0.5433 - val_es_f1: 0.5181 - val_cm_f1: 0.6460\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.3893 - en_loss: 0.6259 - es_loss: 0.4705 - cm_loss: 0.2929 - en_f1: 0.7206 - es_f1: 0.8048 - cm_f1: 0.8905 - val_loss: 3.1507 - val_en_loss: 0.9638 - val_es_loss: 1.0386 - val_cm_loss: 1.1484 - val_en_f1: 0.5417 - val_es_f1: 0.5461 - val_cm_f1: 0.6494\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.3672 - en_loss: 0.6071 - es_loss: 0.4508 - cm_loss: 0.3093 - en_f1: 0.7327 - es_f1: 0.8184 - cm_f1: 0.8833 - val_loss: 3.2157 - val_en_loss: 0.9730 - val_es_loss: 1.0169 - val_cm_loss: 1.2259 - val_en_f1: 0.5265 - val_es_f1: 0.5474 - val_cm_f1: 0.6505\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.3172 - en_loss: 0.6098 - es_loss: 0.4428 - cm_loss: 0.2647 - en_f1: 0.7259 - es_f1: 0.8234 - cm_f1: 0.9056 - val_loss: 3.3258 - val_en_loss: 0.9836 - val_es_loss: 1.0858 - val_cm_loss: 1.2565 - val_en_f1: 0.5289 - val_es_f1: 0.5100 - val_cm_f1: 0.6474\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 1.3282 - en_loss: 0.6134 - es_loss: 0.4670 - cm_loss: 0.2478 - en_f1: 0.7215 - es_f1: 0.8052 - cm_f1: 0.9093 - val_loss: 3.2347 - val_en_loss: 0.9853 - val_es_loss: 1.0544 - val_cm_loss: 1.1950 - val_en_f1: 0.5284 - val_es_f1: 0.5210 - val_cm_f1: 0.6470\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.3942 - en_loss: 0.6262 - es_loss: 0.4516 - cm_loss: 0.3164 - en_f1: 0.7160 - es_f1: 0.8134 - cm_f1: 0.8758 - val_loss: 3.3466 - val_en_loss: 0.9812 - val_es_loss: 1.1202 - val_cm_loss: 1.2452 - val_en_f1: 0.5639 - val_es_f1: 0.5195 - val_cm_f1: 0.6496\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.3399 - en_loss: 0.6066 - es_loss: 0.4409 - cm_loss: 0.2923 - en_f1: 0.7322 - es_f1: 0.8186 - cm_f1: 0.8862 - val_loss: 3.2835 - val_en_loss: 0.9754 - val_es_loss: 1.0797 - val_cm_loss: 1.2285 - val_en_f1: 0.5450 - val_es_f1: 0.5373 - val_cm_f1: 0.6427\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.3059 - en_loss: 0.5962 - es_loss: 0.4437 - cm_loss: 0.2660 - en_f1: 0.7351 - es_f1: 0.8228 - cm_f1: 0.8963 - val_loss: 3.2393 - val_en_loss: 0.9471 - val_es_loss: 1.0638 - val_cm_loss: 1.2284 - val_en_f1: 0.5617 - val_es_f1: 0.5209 - val_cm_f1: 0.6520\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.2994 - en_loss: 0.5947 - es_loss: 0.4293 - cm_loss: 0.2753 - en_f1: 0.7338 - es_f1: 0.8267 - cm_f1: 0.9004 - val_loss: 3.3120 - val_en_loss: 0.9655 - val_es_loss: 1.1013 - val_cm_loss: 1.2452 - val_en_f1: 0.5549 - val_es_f1: 0.5320 - val_cm_f1: 0.6354\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.2815 - en_loss: 0.5812 - es_loss: 0.4259 - cm_loss: 0.2744 - en_f1: 0.7433 - es_f1: 0.8267 - cm_f1: 0.8969 - val_loss: 3.2967 - val_en_loss: 0.9703 - val_es_loss: 1.1055 - val_cm_loss: 1.2209 - val_en_f1: 0.5650 - val_es_f1: 0.5337 - val_cm_f1: 0.6374\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.2559 - en_loss: 0.5870 - es_loss: 0.4113 - cm_loss: 0.2576 - en_f1: 0.7444 - es_f1: 0.8359 - cm_f1: 0.9053 - val_loss: 3.2747 - val_en_loss: 0.9603 - val_es_loss: 1.0862 - val_cm_loss: 1.2281 - val_en_f1: 0.5325 - val_es_f1: 0.5370 - val_cm_f1: 0.6407\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 1.2673 - en_loss: 0.5963 - es_loss: 0.4213 - cm_loss: 0.2497 - en_f1: 0.7360 - es_f1: 0.8282 - cm_f1: 0.9111 - val_loss: 3.4963 - val_en_loss: 0.9577 - val_es_loss: 1.1915 - val_cm_loss: 1.3471 - val_en_f1: 0.5449 - val_es_f1: 0.4991 - val_cm_f1: 0.6497\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 1.2733 - en_loss: 0.5834 - es_loss: 0.4195 - cm_loss: 0.2704 - en_f1: 0.7365 - es_f1: 0.8334 - cm_f1: 0.8973 - val_loss: 3.3498 - val_en_loss: 0.9876 - val_es_loss: 1.0951 - val_cm_loss: 1.2672 - val_en_f1: 0.5290 - val_es_f1: 0.5401 - val_cm_f1: 0.6443\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 1.2535 - en_loss: 0.5906 - es_loss: 0.4193 - cm_loss: 0.2436 - en_f1: 0.7362 - es_f1: 0.8306 - cm_f1: 0.9100 - val_loss: 3.4236 - val_en_loss: 0.9727 - val_es_loss: 1.1015 - val_cm_loss: 1.3494 - val_en_f1: 0.5433 - val_es_f1: 0.5396 - val_cm_f1: 0.6375\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 1.2281 - en_loss: 0.5962 - es_loss: 0.3791 - cm_loss: 0.2527 - en_f1: 0.7322 - es_f1: 0.8508 - cm_f1: 0.9008 - val_loss: 3.5013 - val_en_loss: 0.9851 - val_es_loss: 1.1879 - val_cm_loss: 1.3282 - val_en_f1: 0.5286 - val_es_f1: 0.5313 - val_cm_f1: 0.6300\n",
      "Epoch 91/100\n",
      " 81/200 [===========>..................] - ETA: 32s - loss: 1.2462 - en_loss: 0.5984 - es_loss: 0.3997 - cm_loss: 0.2481 - en_f1: 0.7337 - es_f1: 0.8420 - cm_f1: 0.9078"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-1c696729ddc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_es_x_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_es_x_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_es_x_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_es_y_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_es_y_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_es_y_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = train_generator(32)\n",
    "model.fit_generator(\n",
    "    generator=gen,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=100,\n",
    "    initial_epoch=75,\n",
    "    validation_data=([en_es_x_test,en_es_x_test,en_es_x_test],[en_es_y_test,en_es_y_test,en_es_y_test])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1588352623032514,\n",
       " 0.9760268991767485,\n",
       " 1.0341843986005612,\n",
       " 1.1486239554345317,\n",
       " 0.5573675691789749,\n",
       " 0.5354144665584284,\n",
       " 0.6578083537142195]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([en_es_x_test,en_es_x_test,en_es_x_test],[en_es_y_test,en_es_y_test,en_es_y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! pip3 install keras-self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/IIITH_Codemixed.txt\") as f:\n",
    "    lines = f.readlines()[:-1]\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = {\"0\":-1 , \"1\" :1 , \"2\":0}\n",
    "data = map( lambda x : x.strip().split(\"\\t\") , lines )\n",
    "data = map( lambda x :{'sentiment': sents[x[3]] , 'text': x[1] } , data )\n",
    "hi_en_data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [x['text'] for x in hi_en_data]\n",
    "y_train = [x['sentiment'] for x in hi_en_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "zero_vector = [0 for _ in range(300)]\n",
    "def get_x(data_):\n",
    "    x_  = []\n",
    "    for sent in data_:\n",
    "        pred = list(multibpemb.embed(sent))\n",
    "        if len(pred) >= 32:\n",
    "            pred = pred[:32]\n",
    "        else:\n",
    "            counter = len(pred)\n",
    "            while counter < max_len:\n",
    "                pred.append(zero_vector)\n",
    "                counter = counter + 1\n",
    "        x_.append(pred)\n",
    "    return np.array(x_)\n",
    "X_train =  get_x(X_train)\n",
    "X_test = get_x(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
